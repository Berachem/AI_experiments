"""
Ce script permet de gÃ©nÃ©rer des rÃ©ponses Ã  une question donnÃ©e en utilisant un modÃ¨le de base, puis de les corriger et les amÃ©liorer en utilisant un modÃ¨le plus puissant.
@author: Berachem MARKRIA
"""

import json
import ollama
import time  # UtilisÃ© pour la pause

# ğŸ“Œ Configuration des modÃ¨les
MODEL_GENERATEUR = "llama3.2"   # ModÃ¨le de base Ã  fine-tuner
MODEL_CORRECTEUR = "deepseek-r1:14b"    # ModÃ¨le plus puissant pour amÃ©liorer les rÃ©ponses
N_EXEMPLES = 1            # Nombre de rÃ©ponses gÃ©nÃ©rÃ©es
QUESTION = "Comment collecter lÃ©galement des informations OSINT sur une entreprise ?"  # Sujet

def main():
    # ğŸ“ Liste pour stocker les donnÃ©es de fine-tuning
    dataset = []

    for i in range(N_EXEMPLES):
        print(f"\nğŸ“ GÃ©nÃ©ration de l'exemple {i+1}/{N_EXEMPLES}...")

        # ğŸ”¹ Ã‰tape 1 : GÃ©nÃ©ration par le modÃ¨le Ã  fine-tuner (streaming)
        stream_gen = ollama.chat(
            model=MODEL_GENERATEUR,
            messages=[{"role": "user", "content": QUESTION}],
            stream=True,
        )
        generated_text = ""
        print("ğŸ”° RÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le gÃ©nÃ©rateur "+MODEL_GENERATEUR)
        for chunk in stream_gen:
            generated_text += chunk['message']['content']
            print(chunk['message']['content'], end='', flush=True)
        print()  # Nouvelle ligne aprÃ¨s le streaming

        # ğŸ”¹ Ã‰tape 2 : Correction et amÃ©lioration par le modÃ¨le correcteur (streaming)
        prompt_correcteur = f"""
        Tu es un expert en OSINT. Une IA plus basique a gÃ©nÃ©rÃ© la rÃ©ponse suivante :
        
        {generated_text}

        Corrige et amÃ©liore cette rÃ©ponse en la rendant plus dÃ©taillÃ©e, prÃ©cise, et conforme aux bonnes pratiques OSINT.
        """
        print("âœ¨ Correction et amÃ©lioration par le modÃ¨le correcteur "+MODEL_CORRECTEUR)
        stream_corr = ollama.chat(
            model=MODEL_CORRECTEUR,
            messages=[{"role": "user", "content": prompt_correcteur}],
            stream=True,
        )
        corrected_text = ""
        for chunk in stream_corr:
            corrected_text += chunk['message']['content']
            print(chunk['message']['content'], end='', flush=True)
        print()

        # ğŸ”¹ Ã‰tape 3 : Ajout des donnÃ©es au dataset de fine-tuning
        dataset.append({
            "instruction": QUESTION,
            "generated_response": generated_text,
            "corrected_response": corrected_text
        })

        # Pause pour Ã©viter de surcharger Ollama
        time.sleep(1)

    # ğŸ”¹ Ã‰tape 4 : Sauvegarde du dataset pour le fine-tuning
    dataset_file = "osint_dataset.json"
    with open(dataset_file, "w", encoding="utf-8") as f:
        json.dump(dataset, f, indent=4, ensure_ascii=False)

    print(f"\nğŸ“ Dataset de fine-tuning sauvegardÃ© dans {dataset_file} âœ…")

if __name__ == "__main__":
    main()

